\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{5}
\newif\ifcomments
%\commentsfalse % hide comments
\commentstrue % show comments

% Packages
\usepackage[english]{exercises}
\usepackage{wasysym,hyperref}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}

\begin{document}

\practiceproblems

{\sffamily\noindent
This week's exercises deal with joint distributions, covariance and correlation. You only have to hand in the homework problems; these exercises are optional and for practicing only. If you have questions about them, please post them to the discussion forum and try to help each other. We will also keep an eye on that.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{exercise}[Properties of the Poisson distribution]
	The \emph{Poisson distribution} 	models the number of times some event happens in a given period of time. It's probability mass function is given by
	\[
	P(X = k) = {\frac {\lambda ^{k}e^{-\lambda }}{k!}}, \qquad \lambda \in \mathbb{R}_{>0}, \quad k=0,1,2,\dots
	\]
	where $\lambda$ is the distribution's only parameter. Let $X\sim \text{Poisson}(\lambda)$.
	
	\begin{subex}
		Show that $E[X] = \lambda$.
	\end{subex}
	
	\begin{subex}
		Show that $\Var(X) = \lambda$.
	\end{subex}

\end{exercise}


\begin{exercise}[Maximum a posteriori estimates]
	The \emph{Beta distribution} is a continuous probability distribution over $[0,1]$. You don't need to know anything about continuous distributions for this exercise. The important thing is that if you sample from a Beta distribution, you will get a real number $\theta$ in the interval $[0, 1]$, with probability
	\[
		P(\Theta = \theta|\alpha, \beta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1},
	\] 
	Where $\alpha > 0$ and $\beta > 0$ are the two parameters of the distribution and $B(\alpha, \beta)$ is a normalizing constant that does not depend on $\theta$.

	Let $\Theta \sim \text{Beta}(\alpha, \beta)$. Since $\Theta$ takes values in $[0,1]$ we use $\Theta$ as the parameter for a Bernoulli distribution. Let $X_1, \dots, X_N \sim \text{Bern}(\Theta)$ be i.i.d. random variables --- we use the beta distribution as a \emph{prior} for the Bernoulli.
	
	\begin{subex}
	Check that the joint probability of i.i.d. Bernoulli variables $X_1, \dots X_N$ for a given $\theta$ is given by
	\[
	P(X_1=x_1, \dots, X_N=x_n \mid \theta) = \prod_{i=1}^N \theta^{x_i} (1-\theta)^{1-x_i}
	\]	
	\end{subex}

	\begin{subex}
		Find the unnormalized posterior log-probability 
		\[
			\mathcal P(\theta) := \ln \bigl[ P(X_1=x_1, \dots, X_N=x_n \mid \Theta=\theta) \cdot P(\Theta = \theta| \alpha, \beta)\bigr]
		\]
	\end{subex}

	\begin{subex}
	Find the derivative $\frac{\partial}{\partial \theta} \mathcal P(\theta)$.	
	\end{subex}
	
	\begin{subex}
	Show that the maximum a posteriori estimate 	for $\theta$ is 
	\[
		\theta_\textsc{map} 
			= \text{argmax}_\theta P(\Theta = \theta | X_1, \dots, X_N)
			= \frac{K + \alpha - 1}{N + \alpha + \beta - 2}, 
	\]
	where $K = \sum_{i=1}^N X_i$.
	\end{subex}
	
	\begin{subex}
		For which values of the parameters $\alpha$ and $\beta$ do you get the MLE estimate $\theta_\textsc{ML} = \text{argmax}_\theta P(X_1, \dots X_N \mid \theta)$ back? 
	\end{subex}

	\begin{subex}
		The Beta prior effectively adds some more observations of $X=0$ and $X=1$ to the data. How many of each? 
	\end{subex}


\begin{exercise}[Multiple binomials]
	Let $X_1, \dots, X_N$ be i.i.d.\ $\text{Binom}(n, \theta)$ random variables. 
	Show that the maximum likelihood estimator for $\lambda$ is:
	\[
		\theta_\textsc{mle} = \text{argmax}_\lambda P(X_1, \dots, X_N \mid \lambda) = \frac{\sum_{i=1}^N x_i}{n \cdot N}.
	\] 
	Note that this differs from the script since we now have $N$ rather than 1 random variable. 
	(Be careful not to confuse $n$ and $N$).
	
	\begin{subex}
	Find the log-likelihood $\ln P(X_1^N=x_1^N \mid n, \theta)$ of the data.
	\end{subex}
	
	\begin{subex}
		Find the derivative $\frac{\partial}{\partial \theta} \ln P(X_1^N=x_1^N \mid n, \theta)$.
	\end{subex}
	
	\begin{subex}
		Show that the maximum likelihood estimator for $\theta$ is (keeping $n$ fixed):
		\[
			\theta_\textsc{mle} = \text{argmax}_\theta P(X_1^N=x_1^N \mid n, \theta) = \frac{\sum_{i=1}^N x_i}{n \cdot N}.
		\] 
	\end{subex}
	
\end{exercise}



%	 which is the parameter space for the
%Bernoulli distribution. The Beta distribution is a continuous distribution which we have not discussed in class. For the present exercise this is immaterial, however. The functional form of the
%Beta distribution is: 
%P(p) = \Gamma(\alpha+\beta)/\Gamma(\alpha)\Gamma(\alpha) \times p^{\alpha}(1-p)^{\beta}
%Using the Beta distribution as a prior for the Bernoulli, compute the MAP estimate for the Bernoulli. Also state for which values of \alpha and \beta the MAP estimate is the same as the MLE.
\end{exercise}


\end{document}