\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}

%% Settings
\newcommand\problemset{7}
\newcommand\deadline{Wednesday October 19th, 22:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments

% Packages
\usepackage[english]{{../exercises}}
\usepackage{wasysym,hyperref,graphicx,color,bbm}
\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Var}{Var}

\newcommand{\philip}[1]{\textcolor{red}{[Phil: #1]}}
\newcommand{\chris}[1]{\textcolor{blue}{[Chris: #1]}}
\newcommand{\Ind}[1]{\mathbbm{1}(#1)}
\begin{document}

\homeworkproblems

{\sffamily\noindent
%This week's exercises deal with sets, counting and uniform probabilities.
Your homework must be handed in \textbf{electronically via \href{\canvasURL}{Canvas} before \deadline}. 
This deadline is strict and late submissions are graded with a 0. At the end of the course, the lowest of your 7 weekly homework grades will be dropped. You are strongly encouraged to work together on the exercises, including the homework. However, after this discussion phase, you have to write down and submit your own individual solution. Numbers alone are never sufficient, always motivate your answers.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}[surprisal values (2pt)]
Assume that there is an equal probability of $50\%$ for a person in a population to be male or female. Suppose further that $20\%$ of the males and $6\%$ of the females are tall (height greater than some fixed threshold). Calculate the \emph{surprisal value in bits} of learning that

\begin{subex}
a male person is tall, 
\end{subex}

\begin{subex}
a female person is not tall, 
\end{subex}

\begin{subex}
a tall person is female.
\end{subex}

\end{exercise}

\begin{exercise}[entropy of sum of two dice (2pt)]
Let $X$ encode the sum of the roll of two fair dice. Compute $H(X)$.
\end{exercise}

\begin{exercise}[calculation of relative entropy (2pt)]
Let $X \sim \text{Bernoulli}(p)$ and $Y \sim \text{Bernoulli}(q)$. Compute $D(P_X || P_Y)$ and $D(P_Y || P_X)$ for 

\begin{subex}
$p=q$
\end{subex}

\begin{subex}
$p=\frac 1 3, q=\frac 1 4$.
\end{subex}

\end{exercise}


\begin{exercise}[entropy of independent RVs (4pt)]
\begin{subex}
Prove that if $X$ and $Y$ are independent, we have $H(X,Y)=H(X) + H(Y)$. 
\end{subex}

\begin{subex}
Prove that if $X$ and $Y$ are independent given $Z$, we have $H(X,Y|Z)
= H(X|Z) + H(Y|Z)$. 
\\\emph{Hint: We have encountered conditional
independence in the context of Na{\"i}ve Bayes and Mixture Models
before, look up the corresponding sections in the script for a formal definition.}
\end{subex}

\begin{subex}
Prove that the entropy of $n$ independent RVs is the sum of the entropy of the individual RVs, i.e. that  $H(X_1, ..., X_n) = \sum_{i=1}^{n} H(X_{i}) $ if $P(X_1^n=x_1^n) = \prod_{i = 1}^n P(X_i=x_i)$.
\end{subex}

\begin{subex}
Use the chain rule for entropies and (a) to prove that for
  independent $X$ and $Y$, we have that $H(X|Y) = H(X)$ and $H(Y|X) = H(Y)$.
\end{subex}

\end{exercise}



\end{document}