\documentclass[a4paper,10pt,landscape,twocolumn]{scrartcl}
\newcommand\problemset{7}
\newcommand\deadline{Wednesday October 19th, 22:00h}
\newif\ifcomments
\commentsfalse % hide comments
%\commentstrue % show comments
\usepackage[english]{{../exercises}}
\begin{document}

\homeworkproblems

{\sffamily\noindent
  Your homework must be handed in \textbf{electronically via
  \href{\canvasURL}{Canvas} before \deadline}. This deadline is strict and late
  submissions are graded with a 0. At the end of the course, the lowest of your
  7 weekly homework grades will be dropped. You are strongly encouraged to work
  together on the exercises, including the homework. However, after this
  discussion phase, you have to write down and submit your own individual
  solution. Numbers alone are never sufficient, always motivate your answers.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}[surprisal values (2pt)]
  Assume that there is an equal probability of $50\%$ for a person in a
  population to be male or female. Suppose further that $20\%$ of the males and
  $6\%$ of the females are tall (height greater than some fixed threshold).
  Calculate the \emph{surprisal value in bits} of learning that
  
  % a)  
  \begin{subex}
    a male person is tall, 
  \end{subex}
  
  % b)
  \begin{subex}
    a female person is not tall, 
  \end{subex}
  
  % c)
  \begin{subex}
    a tall person is female.
  \end{subex}
\end{exercise}

%---------------

\begin{exercise}[entropy of sum of two dice (2pt)]
  Let $X$ encode the sum of the roll of two fair dice. Compute $H(X)$.
\end{exercise}

%---------------

\begin{exercise}[calculation of relative entropy (2pt)]
  Let $X \sim \text{Bernoulli}(p)$ and $Y \sim \text{Bernoulli}(q)$. Compute
  $D(P_X || P_Y)$ and $D(P_Y || P_X)$ for
  
  % a)  
  \begin{subex}
    $p=q$
  \end{subex}
  
  % b)
  \begin{subex}
    $p=\frac 1 3, q=\frac 1 4$.
  \end{subex}
\end{exercise}

%---------------

\begin{exercise}[entropy of independent RVs (4pt)]
  
  % a)
  \begin{subex}
    Prove that if $X$ and $Y$ are independent, we have $H(X,Y)=H(X) + H(Y)$. 
  \end{subex}
  
  % b)
  \begin{subex}
    Prove that if $X$ and $Y$ are independent given $Z$, we have 
    $H(X,Y|Z) = H(X|Z) + H(Y|Z)$. 
    \\
    \emph{Hint: We have encountered conditional independence in the context of
    Na{\"i}ve Bayes and Mixture Models before, look up the corresponding
    sections in the script for a formal definition.}
  \end{subex}

  % c)
  \begin{subex}
    Prove that the entropy of $n$ independent RVs is the sum of the entropy of
    the individual RVs, i.e. that  $H(X_1, ..., X_n) = \sum_{i=1}^{n} H(X_{i})$
    if $P(X_1^n=x_1^n) = \prod_{i = 1}^n P(X_i=x_i)$.
  \end{subex}
  
  % d)
  \begin{subex}
    Use the chain rule for entropies and (a) to prove that for independent $X$
    and $Y$, we have that $H(X|Y) = H(X)$ and $H(Y|X) = H(Y)$.
  \end{subex}
\end{exercise}

\end{document}